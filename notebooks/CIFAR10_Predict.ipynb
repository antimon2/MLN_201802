{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10  \n",
    "refer-to: http://blog.suprsonicjetboy.com/entry/2017/04/30/204951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:38.902604Z",
     "start_time": "2018-01-29T07:04:38.892417Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:39.794304Z",
     "start_time": "2018-01-29T07:04:39.790739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:41.931374Z",
     "start_time": "2018-01-29T07:04:40.510554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:41.939223Z",
     "start_time": "2018-01-29T07:04:41.933538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 24\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data (Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:46.418442Z",
     "start_time": "2018-01-29T07:04:46.409033Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_jpg(filepath, size=IMAGE_SIZE):\n",
    "    \"\"\"Reads image from jpg-file.\"\"\"\n",
    "    f = tf.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(f, channels=3)\n",
    "    image = tf.image.resize_images(image, (size, size))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:47.424504Z",
     "start_time": "2018-01-29T07:04:47.418795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    \"\"\"Helper to create a Variable stored on CPU memory.\"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:48.175593Z",
     "start_time": "2018-01-29T07:04:48.160297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_stddev(name, shape, stddev, wd=None):\n",
    "    \"\"\"Helper to create an initialized Variable with truncated-normal initializer.\"\"\"\n",
    "    initializer = tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32)\n",
    "    var = _variable_on_cpu(name, shape, initializer)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:49.925175Z",
     "start_time": "2018-01-29T07:04:49.660939Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, batch_size=None):\n",
    "    \"\"\"Build the CIFAR-10 model.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = images.get_shape()[0].value\n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = _variable_with_stddev('weights', shape=[5, 5, 3, 64], stddev=5e-2)\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        # _activation_summary(conv1)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = _variable_with_stddev('weights', shape=[5, 5, 64, 64], stddev=5e-2)\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        # _activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                           strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = _variable_with_stddev('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        # _activation_summary(local3)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_stddev('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        # _activation_summary(local4)\n",
    "\n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "    # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_stddev('weights', [192, NUM_CLASSES], stddev=1/192.0)\n",
    "        biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        # _activation_summary(softmax_linear)\n",
    "\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:51.037913Z",
     "start_time": "2018-01-29T07:04:51.026866Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(filepath):\n",
    "    \"\"\"Predict file with CIFAR-10 model.\"\"\"\n",
    "    images = [read_jpg(filepath)]\n",
    "    summary_op = tf.summary.image('images', images)\n",
    "    with tf.variable_scope(\"logits\"), tf.name_scope(\"predict\") as scope:\n",
    "        pred_logits = inference(images, batch_size=1)\n",
    "        pred_softmax = tf.nn.softmax(pred_logits)\n",
    "    \n",
    "    return pred_softmax, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:51.986412Z",
     "start_time": "2018-01-29T07:04:51.983092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = './airplane.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`![./airplane.jpg](./airplane.jpg)`\n",
    "\n",
    "![./airplane.jpg](./airplane.jpg)  \n",
    "（Photo by: [フリー素材ぱくたそ](https://www.pakutaso.com/)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:04:55.281338Z",
     "start_time": "2018-01-29T07:04:55.276394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:05:26.670018Z",
     "start_time": "2018-01-29T07:05:26.615248Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(filepath, classes=classes):\n",
    "    checkpoint_dir = './train'\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        logits_op, summary_op = predict(filepath)\n",
    "        # Build an initialization operation to run below.\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Create a loader.\n",
    "        loader = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                # Restores from checkpoint\n",
    "                loader.restore(sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                print('No checkpoint file found')\n",
    "                return\n",
    "\n",
    "            summary_dir = './summary'\n",
    "            summary_predict_dir = os.path.join(summary_dir, 'predict')\n",
    "            summary_writer = tf.summary.FileWriter(summary_predict_dir)\n",
    "\n",
    "            _logits, summary = sess.run([logits_op, summary_op])\n",
    "            summary_writer.add_summary(summary)\n",
    "\n",
    "            cls = classes[np.argmax(_logits)]\n",
    "            print('Prediction:', cls)\n",
    "            print(_logits)\n",
    "            \n",
    "            summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:05:28.259754Z",
     "start_time": "2018-01-29T07:05:27.806564Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`![./pony.jpg](./pony.jpg)`\n",
    "\n",
    "![./pony.jpg](./pony.jpg)  \n",
    "（Photo by: [フリー素材ぱくたそ](https://www.pakutaso.com/)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:05:32.858383Z",
     "start_time": "2018-01-29T07:05:32.507715Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify('./pony.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`![./cat.jpg](./cat.jpg)`\n",
    "\n",
    "![./cat.jpg](./cat.jpg)  \n",
    "（Photo by: [フリー素材ぱくたそ](https://www.pakutaso.com/)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-29T07:05:34.641933Z",
     "start_time": "2018-01-29T07:05:34.236188Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify('./cat.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
